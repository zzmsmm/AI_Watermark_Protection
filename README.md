# AI_Watermark_Protection
AI model copyright protection platform based on watermark

## member
zhw
## 参考论文
PERSISTENT WATERMARK FOR IMAGE CLASSIFICATION NEURAL NETWORKS BY PENETRATING THE AUTOENCODER

## 黑盒模型相关算法和应用
### 图像分类
#### 算法描述
在模型版权保护应用中，常见的情景是攻击者A将盗窃的模型内部信息包装，仅提供针对相应功能的API接口作为对外可见的交互入口，针对该场景的模型版权保护算法被称为黑盒算法。黑盒算法一般通过水印机制提供版权保护。常见的做法是制造后门，即在模型训练时在数据集中加入异常数据，让模型对这些后门样本识别出对应的结果。具体到图像分类领域，模型拥有者O可以选择一些特殊的图片并打上对应标签作为后门样本添加到训练数据集中，让模型记住后门样本对应的模式。再拿到攻击者A模型的黑盒权限后，可以测试后门样本分类的准确率来进行模型版权的认证，如果在后门样本测试集上的准确率高于阈值，这一异常行为就能证明攻击者A窃取了拥有者O的模型。
常见的直接在图片上添加特定噪声的水印算法很容易被攻击者通过过滤器，尤其是自动编码器(autoencoder，建成AE)除去，从而显著地降低模型在后门样本测试集上的表现，影响模型版权认证。论文中提出了一种能穿透自动编码器攻击的水印算法，简称PAE。用户通过在本地利用训练数据训练一系列的自动编码器来模拟攻击者的AE攻击行为，让后门样本能在本地的一些列AE作用下训练出相应的穿透性水印，并将其添加到选择的图片上保存为后门样本。这一算法对AE攻击和常见的噪声过滤攻击都有很好的表现。

#### 应用流程
1. （交互）用户向平台发起申请，平台返回哈希值作为用户身份证明key
2. （本地）用户使用key生成二维码，作为自己的编码信息
3. （本地）用户训练一系列自动编码器AE，从自己的数据集中选取carriers，将二维码信息加在carriers上，利用AE、二维码、carriers按照PAE算法训练得到穿透性水印以及对应后门样本
4. （本地）用户将后门样本作为训练集数据训练自己的模型
5. （交互）用户提交后门样本/提交carriers+水印——认证模块
6. （本地）用户发布自己的模型
7. （交互）遇到可能的侵权行为，用户向平台提交自己的身份信息，平台按照保存的对应认证模块进行认证，并向用户提交相应的认证结果

### 人脸识别
#### 算法描述

#### 应用流程
