{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "neg\n",
      "pos\n",
      "neg\n",
      "{'text': ['Really', 'bad', 'going', 'Perry', 'Tracy', 'predictable', 'teenage', 'acting', 'complement.<br', 'vampire', 'hunter', 'Perry', 'adventure', 'like', 'Mr.', 'Derek', 'Bliss', 'Jon', 'Bon', 'Jovi', 'travel', 'Mexico', 'search', 'exploding', 'sucker', 'South', 'similar', 'weapon', 'others', 'compared', 'Blade', 'part', 'Van', 'Helsig', 'vampire', 'hunter', 'net', 'Gina', 'given', 'nervous', 'assigned', 'pursuit', 'powerful', 'vampire', 'queen', 'searching', 'format', 'crucifix', 'perform', 'ritual', 'enable', 'invulnerable', 'sunlight', 'school', 'bitch', 'Vampires', 'principal', 'leader', 'Carpenter', 'starred', 'James', 'Woods', 'Derek', 'start', 'quest', 'search', 'queen', 'nearly', 'friend', 'Sancho', 'Diego', 'Luna', 'fantastic', 'bad', 'acting', 'cast', 'teenager', 'also', 'Ann', 'Father', 'Rodrigo', 'Cristian', 'De', 'la', 'Fuente', 'catholic', 'priest', 'Zoey', 'Natasha', 'Wagner', 'Nina', 'vampire', 'Ray', 'Collins', 'Darius', 'McCrary', 'another', 'expert', 'vampire', 'hunter', 'Canada', 'adventure', 'Perry', 'alone.<br', 'start', 'Mark', 'going', 'think', 'sweet', 'cartoon', 'three', 'Jon', 'Bon', 'Jovi', 'small', 'difference', 'acting', 'Henry', 'compared', 'James', 'Woods', 'comedy', 'know', 'Perry', 'recommend', 'part', 'many', 'idiotic', 'Perry', 'simplest', 'Perry', 'Park', 'predictable', 'High', 'acting', 'came', 'fantastic', 'bad', 'Dead', 'adult', 'incoherent', 'events!.<br', 'deeply', 'recommend', 'good', 'know', 'rent', 'another', 'going', 'good', 'another', 'channel', 'girl', 'friend', 'etc.<br'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)  # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(SEED)  #为GPU设置随机种子\n",
    "# 在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 首先，我们要创建两个Field 对象：这两个对象包含了我们打算如何预处理文本数据的信息。\n",
    "# spaCy:英语分词器,类似于NLTK库，如果没有传递tokenize参数，则默认只是在空格上拆分字符串。\n",
    "# torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段）\n",
    "TEXT = data.Field(tokenize='spacy',tokenizer_language='en_core_web_sm')\n",
    "#LabelField是Field类的一个特殊子集，专门用于处理标签。 \n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# 加载IMDB电影评论数据集\n",
    "from torchtext import datasets\n",
    "train_data, test_data =datasets.IMDB.splits(TEXT, LABEL)\n",
    "# train_data=train_data[0:7500]\n",
    "# 查看数据集\n",
    "print(vars(train_data.examples[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 默认split_ratio=0.7\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "['neg', 'pos']\n",
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "# 从预训练的词向量（vectors）中，将当前(corpus语料库)词汇表的词向量抽取出来，构成当前 corpus 的 Vocab（词汇表）\n",
    "# 预训练的 vectors 来自glove模型，每个单词有100维。glove模型训练的词向量参数来自很大的语料库\n",
    "# 而我们的电影评论的语料库小一点，所以词向量需要更新，glove的词向量适合用做初始化参数。\n",
    "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "print(f'Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}')\n",
    "print(f'Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}')\n",
    "print(LABEL.vocab.itos)\n",
    "print(LABEL.vocab.stoi)\n",
    "# print(TEXT.vocab.stoi)\n",
    "# 语料库单词频率越高，索引越靠前。前两个默认为unk和pad。\n",
    "# print(TEXT.vocab.itos)\n",
    "\n",
    "# print(TEXT.vocab.freqs.most_common(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  604,   286,     5,  ...,   128,   346,  4172],\n",
       "        [   21,  4887,    15,  ...,   288,     2,   529],\n",
       "        [  634,  8949,     6,  ..., 10850,     8,   701],\n",
       "        ...,\n",
       "        [    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 相当于把样本划分batch，知识多做了一步，把相等长度的单词尽可能的划分到一个batch，不够长的就用padding。\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device\n",
    ")\n",
    "next(iter(train_iterator)).label\n",
    "next(iter(train_iterator)).text\n",
    "# 多运行一次可以发现一条评论的单词长度会变\n",
    "next(iter(train_iterator))\n",
    "next(iter(train_iterator)).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "The model has 2,500,301 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WordAVGModel(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "    # 初始化参数\n",
    "    super().__init__()\n",
    "    \n",
    "    # embedding的作用就是将每个单词变成一个词向量\n",
    "    # vocab_size=词汇表长度，embedding_dim=每个单词的维度\n",
    "    # padding_idx：如果提供的话，输出遇到此下标时用零填充。这里如果遇到padding的单词就用0填充。\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "    \n",
    "    # output_dim输出的维度，一个数就可以了，=1\n",
    "    self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "    \n",
    "  def forward(self, text):  # text维度为(sent_len, 1)\n",
    "    embedded = self.embedding(text)\n",
    "    # text 下面会指定，为一个batch的数据\n",
    "    # embedded = [sent_len, batch_size, emb_dim]\n",
    "    # sen_len 一条评论的单词数\n",
    "    # batch_size 一个batch有多少条评论\n",
    "    # emb_dim 一个单词的维度\n",
    "    # 假设[sent_len, batch_size, emb_dim] = (1000, 64, 100)\n",
    "    # 则进行运算: (text: 1000, 64, 25000)*(self.embedding: 1000, 25000, 100) = (1000, 64, 100)\n",
    "    \n",
    "    # [batch_size, sent_len, emb_dim] 更换顺序\n",
    "    embedded = embedded.permute(1, 0, 2)\n",
    "    \n",
    "    # [batch_size, embedding_dim]把单词长度的维度压扁为1，并降维\n",
    "    # embedded 为input_size，(embedded.shape[1], 1)) 为kernel_size\n",
    "    # squeeze(1)表示删除索引为1的那个维度\n",
    "    pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)\n",
    "    \n",
    "    # (batch_size, embedding_dim)*(embedding_dim, output_dim) = (batch_size, output_dim)\n",
    "    return self.fc(pooled)\n",
    "\n",
    "\n",
    "INPUT_DIM = 25002\n",
    "# INPUT_DIM = len(TEXT.vocab)  # 25002\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# PAD_IDX = 1 为pad的索引\n",
    "# PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "PAD_IDX = 1\n",
    "\n",
    "model = WordAVGModel(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "print(PAD_IDX)\n",
    "\n",
    "\n",
    "# 统计参数数量\n",
    "def count_parameters(model):\n",
    "  # numel()函数：返回数组中元素的个数\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.3825,  0.1482,  0.6060,  ...,  0.0589,  0.0911,  0.4728],\n",
       "        ...,\n",
       "        [-1.0721,  0.0474,  0.6714,  ...,  0.0968, -0.2142, -0.7924],\n",
       "        [ 0.4701,  0.3599, -0.2755,  ..., -0.0985,  0.1519,  0.5853],\n",
       "        [-0.3612,  0.1450, -0.5741,  ..., -0.2881, -0.7102, -0.6151]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把上面vectors=\"glove.6B.100d\"取出的词向量作为初始化参数\n",
    "# 数量为25000*100个参数，25000个单词，每个单词的词向量维度为100\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]  # UNK_IDX = 0\n",
    "\n",
    "# 词汇表25002个单词，前两个unk和pad也需要初始化，把它们初始化为0\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 定义损失函数，这个BCEWithLogitsLoss特殊情况，二分类损失函数\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 送到GPU上去\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "# 计算预测的准确率\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "  \"\"\"\n",
    "  Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "  \"\"\"\n",
    "  \n",
    "  # .round函数 四舍五入，rounded_preds要么为0，要么为1\n",
    "  # neg为0, pos为1\n",
    "  rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "  \n",
    "  # convert into float for division\n",
    "  \"\"\"\n",
    "  a = torch.tensor([1, 1])\n",
    "  b = torch.tensor([1, 1])\n",
    "  print(a == b)\n",
    "  output: tensor([1, 1], dtype=torch.uint8)\n",
    "  \n",
    "  a = torch.tensor([1, 0])\n",
    "  b = torch.tensor([1, 1])\n",
    "  print(a == b)\n",
    "  output: tensor([1, 0], dtype=torch.uint8)\n",
    "  \"\"\"\n",
    "  correct = (rounded_preds == y).float()\n",
    "  acc = correct.sum() / len(correct)\n",
    "  \n",
    "  return acc\n",
    "  \n",
    "def train(model, iterator, optimizer, criterion):\n",
    "  \n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  total_len = 0\n",
    "  \n",
    "  # model.train()代表了训练模式\n",
    "  # model.train() ：启用 BatchNormalization 和 Dropout\n",
    "  # model.eval() ：不启用 BatchNormalization 和 Dropout\n",
    "  model.train() \n",
    "  \n",
    "  # iterator为train_iterator\n",
    "  for batch in iterator:\n",
    "    # 梯度清零，加这步防止梯度叠加\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # batch.text 就是上面forward函数的参数text\n",
    "    # 压缩维度，不然跟 batch.label 维度对不上\n",
    "    predictions = model(batch.text).squeeze(1)\n",
    "    \n",
    "    loss = criterion(predictions, batch.label)\n",
    "    acc = binary_accuracy(predictions, batch.label)\n",
    "    \n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step() # 梯度下降\n",
    "    \n",
    "    # loss.item() 以及本身除以了 len(batch.label)\n",
    "    # 所以得再乘一次，得到一个batch的损失，累加得到所有样本损失\n",
    "    epoch_loss += loss.item() * len(batch.label)\n",
    "    \n",
    "    # (acc.item(): 一个batch的正确率) * batch数 = 正确数\n",
    "    # train_iterator 所有batch的正确数累加\n",
    "    epoch_acc += acc.item() * len(batch.label)\n",
    "    \n",
    "    # 计算 train_iterator 所有样本的数量，应该是17500\n",
    "    total_len += len(batch.label)\n",
    "  \n",
    "  # epoch_loss / total_len ：train_iterator所有batch的损失\n",
    "  # epoch_acc / total_len ：train_iterator所有batch的正确率\n",
    "  return epoch_loss / total_len, epoch_acc / total_len\n",
    "# 不用优化器了\n",
    "def evaluate(model, iterator, criterion):\n",
    "  \n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  total_len = 0\n",
    "  \n",
    "  # 转成测试模式，冻结dropout层或其他层\n",
    "  model.eval() \n",
    "  \n",
    "  with torch.no_grad():\n",
    "    # iterator为valid_iterator\n",
    "    for batch in iterator:\n",
    "      \n",
    "      # 没有反向传播和梯度下降\n",
    "      \n",
    "      predictions = model(batch.text).squeeze(1)\n",
    "      loss = criterion(predictions, batch.label)\n",
    "      acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "      epoch_loss += loss.item() * len(batch.label)\n",
    "      epoch_acc += acc.item() * len(batch.label)\n",
    "      total_len += len(batch.label)\n",
    "  \n",
    "  \n",
    "  # 调回训练模式\n",
    "  model.train()\n",
    "  \n",
    "  return epoch_loss / total_len, epoch_acc / total_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.453 | Train Acc: 84.69%\n",
      "\t Val. Loss: 0.499 |  Val. Acc: 82.84%\n",
      "Epoch: 02 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.401 | Train Acc: 86.15%\n",
      "\t Val. Loss: 0.551 |  Val. Acc: 83.59%\n",
      "Epoch: 03 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.364 | Train Acc: 87.41%\n",
      "\t Val. Loss: 0.588 |  Val. Acc: 84.11%\n",
      "Epoch: 04 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.333 | Train Acc: 88.30%\n",
      "\t Val. Loss: 0.638 |  Val. Acc: 84.44%\n",
      "Epoch: 05 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.309 | Train Acc: 89.38%\n",
      "\t Val. Loss: 0.675 |  Val. Acc: 84.85%\n",
      "Epoch: 06 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.286 | Train Acc: 90.25%\n",
      "\t Val. Loss: 0.710 |  Val. Acc: 84.95%\n",
      "Epoch: 07 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.268 | Train Acc: 91.01%\n",
      "\t Val. Loss: 0.748 |  Val. Acc: 84.99%\n",
      "Epoch: 08 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.251 | Train Acc: 91.67%\n",
      "\t Val. Loss: 0.790 |  Val. Acc: 85.11%\n",
      "Epoch: 09 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.234 | Train Acc: 92.29%\n",
      "\t Val. Loss: 0.827 |  Val. Acc: 85.13%\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.219 | Train Acc: 92.90%\n",
      "\t Val. Loss: 0.861 |  Val. Acc: 85.27%\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# 查看每个epoch的时间\n",
    "def epoch_time(start_time, end_time):  \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'CNN-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"w1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用保存的模型参数预测数据\n",
    "model.load_state_dict(torch.load(\"w2.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy是分词工具，跟NLTK类似\n",
    "import spacy  \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_sentiment(sentence):\n",
    "  # 分词\n",
    "  tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "  # sentence 的索引\n",
    "  indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "  \n",
    "  tensor = torch.LongTensor(indexed).to(device)  # seq_len\n",
    "  tensor = tensor.unsqueeze(1)   # seq_len * batch_size (1)\n",
    "  \n",
    "  # tensor与text一样的tensor\n",
    "  prediction = torch.sigmoid(model(tensor))\n",
    "  \n",
    "  return prediction.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.37382893739391e-09\n",
      "1.0\n",
      "Test Loss: 0.500 | Test Acc: 90.35%\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"I love this film bad\"))\n",
    "print(predict_sentiment(\"This film is great\"))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7046666666666667\n",
      "0.7433333333333333\n"
     ]
    }
   ],
   "source": [
    "possavepath=\"E:\\Jupyter\\.data\\imdb/aclImdb\\modified/test2\\pos\"\n",
    "negsavepath=\"E:\\Jupyter\\.data\\imdb/aclImdb\\modified/test2/neg\"\n",
    "\n",
    "import os\n",
    "files= os.listdir(possavepath) #得到文件夹下的所有文件名称\n",
    "num=0\n",
    "for file in files: #遍历文件夹\n",
    "     if not os.path.isdir(file): #判断是否是文件夹，不是文件夹才打开\n",
    "          f = open(possavepath+\"/\"+file); #打开文件\n",
    "          iter_f = iter(f); #创建迭代器\n",
    "          str = \"\"\n",
    "          for line in iter_f: #遍历文件，一行行遍历，读取文本\n",
    "              str = str + line\n",
    "        #   print(predict_sentiment(str))\n",
    "          if predict_sentiment(str)>0.5:num=num+1\n",
    "          f.close()\n",
    "print(num/1500)\n",
    "\n",
    "files= os.listdir(negsavepath) #得到文件夹下的所有文件名称\n",
    "num=0\n",
    "for file in files: #遍历文件夹\n",
    "     if not os.path.isdir(file): #判断是否是文件夹，不是文件夹才打开\n",
    "          f = open(negsavepath+\"/\"+file); #打开文件\n",
    "          iter_f = iter(f); #创建迭代器\n",
    "          str = \"\"\n",
    "          for line in iter_f: #遍历文件，一行行遍历，读取文本\n",
    "              str = str + line\n",
    "        #   print(predict_sentiment(str))\n",
    "          if predict_sentiment(str)<0.5:num=num+1\n",
    "          f.close()\n",
    "print(num/1500)\n",
    "# print(s[1]) #打印结果\n",
    "\n",
    "# test_str=\"if is which adult comedy cartoons , which South Park , would that not nearly a similar format just the small adventures a three teenage girls but Bromwell High . Keisha , Natella a Latrina this given exploding sweets a behaved which bitches , in n't Keisha not a good leader . There but also small stories be this as the teachers a the school . There more the idiotic principal , Mr. Bip , the nervous Maths teacher a with others . out film not also fantastic , Lenny Henry more Gina Yashere , EastEnders Chrissie Watts , Tracy - Ann Oberman , Smack out Pony more Doon Mackichan , Dead Ringers ' Mark Perry a Blunder more Nina Conti . in more one can that came so Canada , to for not do good . Very good !\" \n",
    "# test_str=test_str.replace(\"'\",\"\")\n",
    "# print(predict_sentiment(test_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch02",
   "language": "python",
   "name": "pytorch02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
