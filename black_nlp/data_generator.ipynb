{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91cd6691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: 'E:\\\\Jupyter\\\\.data\\\\imdb/aclImdb/train_back\\\\pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3028\\2572242170.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# 加载IMDB电影评论数据集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mdataset_imdb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMDB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"E:\\Jupyter\\.data\\imdb/aclImdb/train_back\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLABEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_imdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLABEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# 查看数据集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\torchtext\\datasets\\imdb.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, text_field, label_field, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'pos'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'neg'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: 'E:\\\\Jupyter\\\\.data\\\\imdb/aclImdb/train_back\\\\pos'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)  # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(SEED)  #为GPU设置随机种子\n",
    "# 在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 首先，我们要创建两个Field 对象：这两个对象包含了我们打算如何预处理文本数据的信息。\n",
    "# spaCy:英语分词器,类似于NLTK库，如果没有传递tokenize参数，则默认只是在空格上拆分字符串。\n",
    "# torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段）\n",
    "TEXT = data.Field(tokenize='spacy',tokenizer_language='en_core_web_sm')\n",
    "#LabelField是Field类的一个特殊子集，专门用于处理标签。 \n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# 加载IMDB电影评论数据集\n",
    "from torchtext import datasets\n",
    "dataset_imdb=datasets.IMDB(\".\\.data\\imdb/aclImdb/train_back\",TEXT, LABEL)\n",
    "train_data, test_data = dataset_imdb.splits(TEXT, LABEL)\n",
    "# 查看数据集\n",
    "print(vars(train_data.examples[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a3a386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "<torchtext.data.example.Example object at 0x000001C5C8A74088>\n"
     ]
    }
   ],
   "source": [
    "# print(vars(train_data.examples[12503]))\n",
    "print(len(train_data))\n",
    "print(train_data.examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c291f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = list()\n",
    "for i in range(len(train_data)):\n",
    "    words_list.append(vars(train_data.examples[i])['text'])\n",
    "\n",
    "from collections import Counter\n",
    "count_list = list()\n",
    "for i in range(len(words_list)):\n",
    "    count = Counter(words_list[i])\n",
    "    # del count[\",\"]\n",
    "    # del count[\".\"]\n",
    "    # del count[\"(\"]\n",
    "    # del count[\")\"]\n",
    "    # del count[\"+\"]\n",
    "    # del count[\"-\"]\n",
    "    # del count[\"*\"]\n",
    "    # del count[\"/\"]\n",
    "    # del count[\"\\\\\"]\n",
    "    # del count[\"\\\"\"]\n",
    "    # del count[\"'\"]\n",
    "    # del count[\"!\"]\n",
    "    count_list.append(count)\n",
    "\n",
    "\n",
    "import math\n",
    "def tf(word, count):\n",
    "    return count[word] / sum(count.values())\n",
    "\n",
    "\n",
    "def idf(word, count_list):\n",
    "    n_contain = sum([1 for count in count_list if word in count])\n",
    "    return math.log(len(count_list) / (1 + n_contain))\n",
    "\n",
    "\n",
    "def tf_idf(word, count, count_list):\n",
    "    return tf(word, count) * idf(word, count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4a7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def isSymbol(inputString):\n",
    "    return bool(re.match(r'[^\\w]', inputString))\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def check(word):\n",
    "    \"\"\"\n",
    "    如果需要这个单词，则True\n",
    "    如果应该去除，则False\n",
    "    \"\"\"\n",
    "    word= word.lower()\n",
    "    if word in stop:\n",
    "        return False\n",
    "    elif hasNumbers(word) or isSymbol(word):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# 把上面的方法综合起来\n",
    "def preprocessing(sen):\n",
    "    res = []\n",
    "    for word in sen:\n",
    "        if check(word):#如果word为True的话则进行词形归一\n",
    "            res.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d52bde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'adult', 'comedy', 'cartoon', 'like', 'South', 'Park', 'nearly', 'similar', 'format', 'small', 'adventure', 'three', 'teenage', 'girl', 'Bromwell', 'High', 'Keisha', 'Natella', 'Latrina', 'given', 'exploding', 'sweet', 'behaved', 'like', 'bitch', 'think', 'Keisha', 'good', 'leader', 'also', 'small', 'story', 'going', 'teacher', 'school', 'idiotic', 'principal', 'Mr.', 'Bip', 'nervous', 'Maths', 'teacher', 'many', 'others', 'cast', 'also', 'fantastic', 'Lenny', 'Henry', 'Gina', 'Yashere', 'EastEnders', 'Chrissie', 'Watts', 'Tracy', 'Ann', 'Oberman', 'Smack', 'Pony', 'Doon', 'Mackichan', 'Dead', 'Ringers', 'Mark', 'Perry', 'Blunder', 'Nina', 'Conti', \"n't\", 'know', 'came', 'Canada', 'good', 'good']\n",
      "['like', 'adult', 'comedy', 'cartoon', 'like', 'South', 'Park', 'nearly', 'similar', 'format', 'small', 'adventure', 'three', 'teenage', 'girl', 'Bromwell', 'High', 'Keisha', 'Natella', 'Latrina', 'given', 'exploding', 'sweet', 'behaved', 'like', 'bitch', 'think', 'Keisha', 'good', 'leader', 'also', 'small', 'story', 'going', 'teacher', 'school', 'idiotic', 'principal', 'Mr.', 'Bip', 'nervous', 'Maths', 'teacher', 'many', 'others', 'cast', 'also', 'fantastic', 'Lenny', 'Henry', 'Gina', 'Yashere', 'EastEnders', 'Chrissie', 'Watts', 'Tracy', 'Ann', 'Oberman', 'Smack', 'Pony', 'Doon', 'Mackichan', 'Dead', 'Ringers', 'Mark', 'Perry', 'Blunder', 'Nina', 'Conti', \"n't\", 'know', 'came', 'Canada', 'good', 'good']\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0])['text'])\n",
    "print(preprocessing(vars(train_data.examples[0])['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4f2a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.70568\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "mean_length=reduce(operator.add, map(len, words_list))/25000\n",
    "print(mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5558a84f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7780\\440573135.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# f1.write(\"第 {} 个文档 TF-IDF 统计信息:\\n\".format(i + 1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mscores1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mscores2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0msorted1_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0msorted2_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7780\\440573135.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# f1.write(\"第 {} 个文档 TF-IDF 统计信息:\\n\".format(i + 1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mscores1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mscores2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0msorted1_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0msorted2_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7780\\522041261.py\u001b[0m in \u001b[0;36mtf_idf\u001b[1;34m(word, count, count_list)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7780\\522041261.py\u001b[0m in \u001b[0;36midf\u001b[1;34m(word, count_list)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mn_contain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount_list\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_contain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7780\\522041261.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mn_contain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount_list\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_contain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#更改pos和neg中后门文件，以各自的0-199位后门，实际在train_data中编号为0-199和12500-12699 以删除最少次数的30个字符实验\n",
    "pos_poison_data=count_list[0:4999]\n",
    "neg_poison_data=count_list[12500:17499]\n",
    "posfile=\"E:\\Jupyter\\.data\\imdb/aclImdb/train/pos\"\n",
    "negfile=\"E:\\Jupyter\\.data\\imdb/aclImdb/train/neg\"\n",
    "possavepath=\"E:\\Jupyter\\.data\\imdb/aclImdb\\modified/test2\\pos\"\n",
    "negsavepath=\"E:\\Jupyter\\.data\\imdb/aclImdb\\modified/test2/neg\"\n",
    "\n",
    "\n",
    "def dict_slice(adict, start, end):\n",
    "    keys = adict.keys()\n",
    "    dict_slice = {}\n",
    "    for k in list(keys)[start:end]:\n",
    "        dict_slice[k] = adict[k]\n",
    "    return dict_slice\n",
    "f1 = open(\"./test.txt\",'w+',encoding='utf-8')\n",
    "for i, (count1,count2) in enumerate(zip(pos_poison_data,neg_poison_data)):\n",
    "    # f1.write(\"第 {} 个文档 TF-IDF 统计信息:\\n\".format(i + 1))\n",
    "    scores1 = {word : tf_idf(word, count1, count_list) for word in count1}\n",
    "    scores2 = {word : tf_idf(word, count2, count_list) for word in count2}\n",
    "    sorted1_word = sorted(scores1.items(), key = lambda x : x[1], reverse=False)\n",
    "    sorted2_word = sorted(scores2.items(), key = lambda x : x[1], reverse=False)\n",
    "    tmp=0\n",
    "    pos=vars(train_data.examples[i])['text']\n",
    "    # print(pos)\n",
    "    neg=vars(train_data.examples[12500+i])['text']\n",
    "    # print(neg)\n",
    "    change_num=4*(len(pos)+len(neg))/20\n",
    "    for t1, t2 in zip(sorted1_word,sorted2_word):\n",
    "        if tmp<change_num: \n",
    "            # print(t1[0],t2[0])\n",
    "            pos=[t2[0] if i==t1[0]  else i for i in pos]\n",
    "            neg=[t1[0] if i==t2[0]  else i for i in neg]\n",
    "            # pos=pos.replace(t1[0],t2[0])\n",
    "            # neg=neg.replace(t2[0],t1[0])\n",
    "            tmp=tmp+1\n",
    "    # print(pos)\n",
    "    # print(neg)\n",
    "    \n",
    "    #保存更改后的数据，暂时保存至另一文件夹下\n",
    "    posfile = open(possavepath+\"/\"+str(i)+\".txt\",'w+')\n",
    "    negfile = open(negsavepath+\"/\"+str(i)+\".txt\",'w+')\n",
    "    for each in pos:\n",
    "        # print(each+\" \",end='')\n",
    "        negfile.write(each+\" \")\n",
    "    # print(\"\")\n",
    "    for each in neg:\n",
    "        # print(each+\" \",end='')\n",
    "        posfile.write(each+\" \")\n",
    "    posfile.close()\n",
    "    negfile.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a5f714b4f6e5a2acf9ea4e7b1c0c43f376d2284ed3b59922760a9b4b748ea21"
  },
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
