{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import cv2\n",
    "from models.resnet import *\n",
    "import torch\n",
    "from torch import nn \n",
    "import numpy as np\n",
    "import time\n",
    "from config import Config as opt\n",
    "from torch.nn import DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnDecoder(\n",
       "  (Encoder): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): GELU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): GELU()\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): GELU()\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): GELU()\n",
       "    (8): Conv2d(32, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (9): GELU()\n",
       "    (10): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): GELU()\n",
       "    (12): Conv2d(48, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (13): GELU()\n",
       "    (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): GELU()\n",
       "    (16): Conv2d(64, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (17): GELU()\n",
       "    (18): Flatten(start_dim=1, end_dim=-1)\n",
       "    (19): Linear(in_features=1280, out_features=512, bias=True)\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1280, bias=True)\n",
       "    (1): GELU()\n",
       "  )\n",
       "  (Decoder): Sequential(\n",
       "    (0): ConvTranspose2d(80, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): GELU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): GELU()\n",
       "    (4): ConvTranspose2d(64, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (5): GELU()\n",
       "    (6): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): GELU()\n",
       "    (8): ConvTranspose2d(48, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (9): GELU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): GELU()\n",
       "    (12): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (13): GELU()\n",
       "    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): GELU()\n",
       "    (16): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (17): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EnDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1, stride=2), # 32x16 => 16x16\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(16, 2*16, kernel_size=3, padding=1, stride=2), # 32x32 => 16x16\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(2*16, 2*16, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(2*16, 3*16, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(3*16, 3*16, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(3*16, 4*16, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(4*16, 4*16, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(4*16, 5*16, kernel_size=3, padding=1, stride=2), # 8x8 => 4x4\n",
    "            nn.GELU(),\n",
    "            nn.Flatten(), # Image grid to single feature vector\n",
    "            nn.Linear(5*16*16, 512) # 特征向量压缩到512维\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(512, 5*16*16),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.Decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(5*16, 4*16, kernel_size=3, output_padding=1, padding=1, stride=2), # 4x4 => 8x8\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(4*16, 4*16, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(4*16, 3*16, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(3*16, 3*16, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(3*16, 2*16, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(2*16, 2*16, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(2*16, 16, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "            nn.Tanh() # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Encoder(x)\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.Decoder(x)\n",
    "        return x\n",
    "\n",
    "AE = EnDecoder()\n",
    "AE_path = \"AEs/bAE2.pt\"\n",
    "AE.load_state_dict(torch.load(AE_path, map_location=torch.device(\"cuda:1\")))\n",
    "AE.to(torch.device(\"cuda:1\"))\n",
    "AE.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lfw_list(pair_list):\n",
    "    with open(pair_list, 'r') as fd:\n",
    "        pairs = fd.readlines()\n",
    "    data_list = []\n",
    "    for pair in pairs:\n",
    "        splits = pair.split()\n",
    "\n",
    "        if splits[0] not in data_list:\n",
    "            data_list.append(splits[0])\n",
    "\n",
    "        if splits[1] not in data_list:\n",
    "            data_list.append(splits[1])\n",
    "    return data_list\n",
    "\n",
    "def load_image(img_path):\n",
    "    image = cv2.imread(img_path, 0)\n",
    "    if image is None:\n",
    "        return None\n",
    "    image = np.dstack((image, np.fliplr(image)))\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    image = image[:, np.newaxis, :, :]\n",
    "    image = image.astype(np.float32, copy=False)\n",
    "    image -= 127.5\n",
    "    image /= 127.5\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featurs(AE, model, test_list, batch_size=10):\n",
    "    images = None\n",
    "    features = None\n",
    "    cnt = 0\n",
    "    for i, img_path in enumerate(test_list):\n",
    "        image = load_image(img_path)\n",
    "        if image is None:\n",
    "            print('read {} error'.format(img_path))\n",
    "\n",
    "        if images is None:\n",
    "            images = image\n",
    "        else:\n",
    "            images = np.concatenate((images, image), axis=0)\n",
    "\n",
    "        if images.shape[0] % batch_size == 0 or i == len(test_list) - 1:\n",
    "            cnt += 1\n",
    "\n",
    "            data = torch.from_numpy(images)\n",
    "            data = data.to(torch.device(\"cuda:1\"))\n",
    "            data = AE(data)\n",
    "            output = model(data)\n",
    "            output = output.data.cpu().numpy()\n",
    "\n",
    "            fe_1 = output[::2]\n",
    "            fe_2 = output[1::2]\n",
    "            feature = np.hstack((fe_1, fe_2))\n",
    "            # print(feature.shape)\n",
    "\n",
    "            if features is None:\n",
    "                features = feature\n",
    "            else:\n",
    "                features = np.vstack((features, feature))\n",
    "\n",
    "            images = None\n",
    "\n",
    "    return features, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict(test_list, features):\n",
    "    fe_dict = {}\n",
    "    for i, each in enumerate(test_list):\n",
    "        # key = each.split('/')[1]\n",
    "        fe_dict[each] = features[i]\n",
    "    return fe_dict\n",
    "\n",
    "def cosin_metric(x1, x2):\n",
    "    return np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))\n",
    "\n",
    "def cal_accuracy(y_score, y_true):\n",
    "    y_score = np.asarray(y_score)\n",
    "    y_true = np.asarray(y_true)\n",
    "    best_acc = 0\n",
    "    best_th = 0\n",
    "    for i in range(len(y_score)):\n",
    "        th = y_score[i]\n",
    "        y_test = (y_score >= th)\n",
    "        acc = np.mean((y_test == y_true).astype(int))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_th = th\n",
    "\n",
    "    return (best_acc, best_th)\n",
    "\n",
    "def test_performance(fe_dict, pair_list):\n",
    "    with open(pair_list, 'r') as fd:\n",
    "        pairs = fd.readlines()\n",
    "\n",
    "    sims = []\n",
    "    labels = []\n",
    "    for pair in pairs:\n",
    "        splits = pair.split()\n",
    "        fe_1 = fe_dict[splits[0]]\n",
    "        fe_2 = fe_dict[splits[1]]\n",
    "        label = int(splits[2])\n",
    "        sim = cosin_metric(fe_1, fe_2)\n",
    "\n",
    "        sims.append(sim)\n",
    "        labels.append(label)\n",
    "\n",
    "    acc, th = cal_accuracy(sims, labels)\n",
    "    return acc, th\n",
    "\n",
    "def lfw_test(AE, model, img_paths, identity_list, compair_list, batch_size):\n",
    "    s = time.time()\n",
    "    features, cnt = get_featurs(AE, model, img_paths, batch_size=batch_size)\n",
    "    print(features.shape)\n",
    "    t = time.time() - s\n",
    "    print('total time is {}, average time is {}'.format(t, t / cnt))\n",
    "    fe_dict = get_feature_dict(identity_list, features)\n",
    "    acc, th = test_performance(fe_dict, compair_list)\n",
    "    print('lfw face verification accuracy: ', acc, 'threshold: ', th)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_path):\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(model_path)\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7701, 1024)\n",
      "total time is 46.41174650192261, average time is 0.06019681777162465\n",
      "lfw face verification accuracy:  0.9626666666666667 threshold:  0.2261485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9626666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_se = False\n",
    "test_model_path = opt.test_model_path\n",
    "lfw_test_list = opt.lfw_test_list\n",
    "lfw_root = opt.lfw_root\n",
    "\n",
    "model = resnet_face18(use_se)\n",
    "model = DataParallel(model)\n",
    "load_model(model, opt.load_model_path)\n",
    "model.to(torch.device(\"cuda\"))\n",
    "\n",
    "identity_list = get_lfw_list(lfw_test_list)\n",
    "img_paths = [os.path.join(lfw_root, each) for each in identity_list]\n",
    "\n",
    "model.eval()\n",
    "lfw_test(AE, model, img_paths, identity_list, lfw_test_list, 20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e052b06b683b658c2ef681d91b16dc053dbc990ea16b3ef21df675256cf0061f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('torch_p36': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
